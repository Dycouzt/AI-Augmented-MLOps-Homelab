# Project Master Context: AI-Augmented, Self-Healing MLOps Platform
## Hybrid Cloud-Edge Architecture

## 1. Project Vision & Core Purpose

The primary vision is to build a fully automated, hybrid MLOps platform that manages the entire lifecycle of a machine learning model, from code commit to production deployment and monitoring.

The project's unique differentiator is its **multi-domain AI-Augmented operational layer**. The platform uses Google's Gemini LLM to perform intelligent:
- Real-time log analysis and root cause diagnosis
- Model drift detection and remediation recommendations
- CI/CD pipeline failure triage with actionable fixes

This demonstrates a forward-thinking approach to AIOps (AI for IT Operations) within an MLOps context, showcasing how AI can reduce MTTR, improve reliability, and accelerate development velocity.

**Architecture Philosophy:** This platform demonstrates cloud-agnostic hybrid architecture principles. The control plane runs on GCP (chosen for its Kubernetes heritage and Gemini integration), while the data plane runs on-premises. All infrastructure is defined in portable Kubernetes manifests, making the platform migratable to AWS, Azure, or fully on-premises environments.

This project serves as a portfolio centerpiece to showcase advanced, practical skills in Platform Engineering, MLOps, DevSecOps, Hybrid Cloud Architecture, and applied AI.

## 2. Key Objectives & Showcase Goals

This project is designed to demonstrate the following key competencies to potential employers:

- **Hybrid Cloud Architecture:** Ability to design and operate distributed systems spanning cloud and on-premises infrastructure
- **Cloud-Native & Orchestration Mastery:** Deep, practical knowledge of Kubernetes as a portable, cloud-agnostic orchestration platform
- **Modern DevOps Best Practices:** Strict adherence to GitOps principles using ArgoCD as the single source of truth for the cluster's state
- **Advanced CI/CT/CD Pipeline Engineering:** The ability to design and implement a sophisticated pipeline that handles the unique requirements of ML models, including Continuous Training (CT) and validation
- **Integrated Security (DevSecOps):** Security is not an afterthought. The pipeline MUST include automated SAST, SCA, and container vulnerability scanning at every stage
- **ML Lifecycle Management:** Understanding of the tools (MLflow, KServe) and processes required to version, deploy, and serve ML models effectively
- **Multi-Domain AIOps Integration:** 
  - Intelligent log analysis and alert contextualization
  - Automated drift detection and model health diagnostics
  - CI/CD failure triage with actionable remediation steps
  - Demonstrates practical application of LLMs to reduce MTTR and improve operational efficiency

## 3. Core Architecture & End-to-End Workflow

The platform operates in three distinct phases:

### Phase 1: The Platform Foundation (Hybrid Kubernetes Cluster)

#### Cloud Control Plane (GCP e2-small)
1. **GCP VM Provisioning:** A single e2-small instance (2 vCPU, 2GB RAM) in us-west1-b running Ubuntu 22.04 LTS
2. **K3s Server Installation:** The VM runs k3s in server mode, hosting the Kubernetes control plane components (API server, scheduler, controller manager, etcd)
3. **Firewall Configuration:** GCP firewall rules allow inbound traffic on port 6443 (Kubernetes API) from the on-premises worker
4. **GitOps Controller:** ArgoCD is installed into the cluster and configured to monitor a dedicated Git repository (the "infra-manifests" repo). ALL subsequent platform components are deployed and managed by ArgoCD

#### On-Premises Data Plane (Windows Laptop)
1. **WSL2 Setup:** Windows Subsystem for Linux 2 with Ubuntu 22.04
2. **K3s Agent Installation:** WSL2 runs k3s in agent mode, connecting to the GCP control plane via the k3s VPN tunnel
3. **Workload Execution:** This node runs all ML inference workloads (KServe model serving pods)
4. **24/7 Availability:** Dedicated to the platform, provides consistent compute for production workloads

#### Mac Developer Workstation (Optional)
- kubectl configured to access the cluster
- Git client for code commits
- VS Code for development
- Does NOT run any Kubernetes components

### Phase 2: The MLOps CI/CT/CD Pipeline (GitHub Actions)

1. **Trigger:** A "Data Scientist" persona pushes new model source code, a test dataset, and a `Dockerfile` to a specific GitHub repository (the "model-source" repo)

2. **Continuous Integration (CI):**
   - Code quality checks (Flake8) and unit tests (Pytest) are executed
   - Security scans are performed: SAST (Bandit), SCA (Snyk), and Secrets Detection (Gitleaks)

3. **Continuous Training (CT):**
   - The pipeline executes the model training script
   - The trained model is evaluated against a test dataset. If its accuracy score is below a predefined threshold, the pipeline fails
   - The validated model artifact and its performance metrics are versioned and logged in the MLflow server running on the cluster

4. **Containerization & Security:**
   - A container image is built using the `Dockerfile`
   - The container image is scanned for OS and library vulnerabilities using Trivy. The pipeline fails if critical vulnerabilities are found
   - The clean image is pushed to a container registry (GitHub Container Registry)

5. **Continuous Deployment (CD) via GitOps Handoff:**
   - The final step of the GitHub Actions workflow does NOT deploy directly to Kubernetes
   - Instead, it automatically makes a commit to the "infra-manifests" repo, updating the Kubernetes deployment manifest to point to the new container image tag
   - ArgoCD detects this change in the manifest repo and automatically synchronizes the cluster, rolling out the new model version using the KServe model server

6. **AI-Augmented Failure Handling:**
   - If ANY step in the pipeline fails, an automated triage workflow is triggered
   - The triage workflow invokes Gemini with failure-specific context (logs, error messages, code snippets)
   - AI-generated analysis is posted as a commit comment or GitHub Issue
   - Developers receive immediate, expert-level guidance without manual log analysis

### Phase 3: The AI-Augmented Monitoring & Operations Loop

#### Real-Time Log Analysis
1. **Alert Condition:** The Prometheus monitoring stack detects an anomaly (e.g., model inference latency exceeds 500ms, pod crash loop, OOMKilled events)
2. **Alert Firing:** Prometheus sends an alert to Alertmanager
3. **Webhook Trigger:** Alertmanager fires a webhook to GitHub Actions `workflow_dispatch` endpoint. The webhook payload contains the full JSON context of the alert
4. **AI Analyst Workflow (GitHub Actions):**
   - The workflow parses the alert JSON
   - Uses `kubectl` with cluster credentials to fetch logs from the specific pod identified in the alert
   - Invokes Gemini CLI (`gemini chat`) with a structured prompt containing alert data and logs
   - **Prompt Template:** "You are an expert MLOps SRE. A high-latency alert has been triggered for a model serving pod. Based on the following alert data and pod logs, provide a brief markdown report with the 3 most likely root causes and a specific, actionable remediation step for each."
5. **Intelligent Reporting:** The markdown-formatted analysis is posted to a Slack channel or GitHub Issue, providing the on-call engineer with immediate, intelligent context

#### AI-Powered Model Drift Detection
1. **Trigger Condition:** Prometheus detects model performance degradation:
   - Accuracy drops below defined threshold (e.g., <85%)
   - Prediction distribution shift detected
   - Increased prediction confidence variance
2. **Automated Context Gathering:**
   - Fetch current model metrics from MLflow (accuracy, precision, recall, F1)
   - Retrieve historical performance baselines (last 7, 14, 30 days)
   - Pull feature importance statistics from the current model version
   - Gather inference request metadata (payload distributions, timestamp patterns)
3. **AI Drift Analysis Workflow (GitHub Actions):**
   - Alertmanager webhook triggers a dedicated `drift-analysis.yml` workflow
   - The workflow aggregates all metrics and constructs a structured prompt
   - **Prompt Template:** "You are an expert ML model monitoring specialist. A production model is experiencing performance degradation. Based on the following metrics comparison [baseline vs current], feature distributions, and temporal patterns, provide: (1) Root cause hypothesis (data drift, concept drift, or model decay), (2) Affected features or data segments, (3) Recommended remediation (retrain, rollback, or feature engineering), (4) Urgency level (P0-P2)."
   - Gemini returns a structured markdown analysis
4. **Intelligent Response Actions:**
   - **High-Confidence Drift (P0):** Automatically trigger a retraining pipeline with updated data
   - **Medium-Confidence (P1):** Create a GitHub Issue with analysis and tag the ML team
   - **Low-Confidence (P2):** Post analysis to Slack with "investigate" recommendation
   - All actions include the AI-generated hypothesis and supporting metrics

#### AI-Powered Pipeline Failure Triage
1. **Trigger Condition:** Any GitHub Actions workflow failure in the model-source repository:
   - CI stage failures (linting, testing)
   - Security scan failures (SAST, SCA, secrets, container scanning)
   - Training failures (model validation below threshold)
   - Deployment failures (manifest validation, ArgoCD sync errors)
2. **Automated Context Gathering:**
   - Parse the GitHub Actions workflow run logs
   - Extract the specific failing step and error messages
   - Collect relevant file contents if the failure is code-related (e.g., the file flagged by Bandit)
   - Retrieve the last successful run's configuration for comparison
3. **AI Triage Workflow (GitHub Actions):**
   - A `on: workflow_run` trigger activates when any workflow completes with `conclusion: failure`
   - The triage workflow fetches all failure context via GitHub API
   - **Prompt Templates (vary by failure type):**
     - **Security Scan Failure:** "You are a DevSecOps specialist. A [SAST/SCA/Container] security scan has failed with the following findings: [scan output]. For each finding: (1) Explain the vulnerability in simple terms, (2) Assess actual risk in this ML context, (3) Provide specific code fix with line numbers, (4) Indicate if it's a false positive."
     - **Test Failure:** "You are a Python testing expert. Unit tests failed with: [pytest output]. Analyze: (1) Root cause of failure, (2) Whether it's a code bug or test issue, (3) Specific fix for the failing test/code."
     - **Model Validation Failure:** "You are an ML engineer. Model training completed but accuracy is [X]%, below threshold [Y]%. Based on training logs: [logs], suggest: (1) Likely causes (data quality, hyperparameters, overfitting), (2) Top 3 specific actions to improve performance."
   - Gemini returns a detailed, actionable triage report
4. **Automated Developer Feedback:**
   - Post the AI analysis as a comment directly on the failing commit or PR
   - Create a detailed GitHub Issue if the failure is in the main branch
   - Send a Slack notification with summary + link to full analysis
   - Tag relevant team members based on failure type (security team for CVEs, ML team for validation failures)

**Key Design Principle:** The AI does NOT auto-fix code or auto-merge changes. It accelerates human decision-making by providing expert-level analysis and specific, actionable recommendations. The human developer remains in control of all remediation actions.

## 4. Technology Stack

**Cloud Infrastructure:**
- **Cloud Provider:** Google Cloud Platform (GCP)
- **Compute:** e2-small instance (2 vCPU, 2GB RAM, Ubuntu 22.04 LTS)
- **Region:** us-west1-b (Free tier eligible, low latency to West Coast)

**On-Premises Infrastructure:**
- **OS:** Windows 11 with WSL2 (Ubuntu 22.04)
- **Runtime:** Docker Desktop (for WSL2 integration)

**Orchestration & Platform:**
- **Kubernetes Distribution:** K3s (lightweight, production-ready)
- **Control Plane:** GCP e2-small (k3s server mode)
- **Data Plane:** Windows WSL2 (k3s agent mode)
- **GitOps Controller:** ArgoCD
- **Containerization:** Docker / Buildx

**CI/CD & Automation:**
- **CI/CD Platform:** GitHub Actions (free for public repos)
- **Container Registry:** GitHub Container Registry (GHCR)

**MLOps Tooling:**
- **Model Registry:** MLflow (tracking server + artifact store)
- **Model Serving:** KServe (Kubernetes-native inference)

**DevSecOps Tooling:**
- **SAST:** Bandit (Python security linter)
- **SCA:** Snyk (dependency vulnerability scanning)
- **Secrets Detection:** Gitleaks
- **Container Scanning:** Trivy (OS + library CVEs)

**Monitoring & Alerting:**
- **Metrics Collection:** Prometheus
- **Visualization:** Grafana
- **Alert Routing:** Alertmanager
- **Webhook Integration:** GitHub Actions workflow_dispatch

**AI Integration:**
- **LLM Provider:** Google Gemini API
- **Interface:** Gemini API (REST) via Python SDK
- **Use Cases:**
  - Real-time log analysis for production alerts
  - Model drift detection and root cause hypothesis
  - CI/CD pipeline failure triage and remediation guidance
  - Automated incident report generation

## 5. Guiding Principles

- **Infrastructure as Code (IaC):** All infrastructure and platform configuration MUST be defined declaratively in code and stored in Git
- **GitOps is the Source of Truth:** The state of the Kubernetes cluster MUST reflect the state of the main branch of the manifests repository. No manual `kubectl apply` commands
- **Automation First:** Every repetitive task, from testing to deployment and analysis, should be automated
- **Immutable Infrastructure:** Containers are treated as immutable artifacts. To change a running application, a new image is built and deployed
- **Security is Shift-Left:** Security checks are integrated into the earliest possible stages of the pipeline
- **Cloud-Agnostic Design:** Despite using GCP, the platform is architected with portability in mind. All infrastructure is Kubernetes-native (no GCP-specific services like GKE, Cloud Run, etc.). The platform can be migrated to AWS, Azure, or fully on-premises by changing only the control plane VM
- **Hybrid Architecture:** Demonstrates real-world patterns where control planes run in the cloud for management simplicity, while data planes run on-premises for data sovereignty, cost optimization, or latency requirements

## 6. Project Timeline & Milestones

**Total Duration:** 2 weeks

**Week 1: Foundation & CI/CD**
- Day 1-2: GCP VM setup, k3s control plane, Windows worker join
- Day 3-4: ArgoCD installation, MLflow deployment, basic monitoring
- Day 5-7: GitHub Actions CI/CT/CD pipeline, security scanning integration

**Week 2: AIOps & Polish**
- Day 8-9: Prometheus alerting rules, Gemini log analysis workflow
- Day 10-11: Model drift detection, pipeline triage workflows
- Day 12-14: Testing, documentation, demo preparation, teardown preparation

## 7. Cost Analysis

**GCP Costs (e2-small, us-west1):**
- Hourly Rate: ~$0.018/hour
- Daily Cost: ~$0.43/day
- 2-Week Cost: ~$6.50 total
- Monthly Cost (if extended): ~$13/month

**Other Costs:**
- GitHub Actions: $0 (free for public repos)
- Gemini API: $0 (free tier: 15 requests/min, 1500 requests/day)
- Domain/SSL: $0 (using cluster IPs or ngrok for demos)

**Total Project Cost: ~$6.50**

## 8. Teardown Strategy

When the project is complete:
1. Delete the GCP e2-small instance (stops billing immediately)
2. Uninstall k3s from Windows WSL2: `sudo /usr/local/bin/k3s-agent-uninstall.sh`
3. Archive Git repositories (make them private or delete)
4. Export metrics/logs/screenshots for portfolio documentation
5. Document lessons learned in project README

**No ongoing costs after teardown.**
